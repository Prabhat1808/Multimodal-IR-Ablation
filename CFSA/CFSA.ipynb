{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CFSA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "3vSMOjqMFa2b",
        "outputId": "a0752297-f138-4f45-d1d5-367c89fccc2a"
      },
      "source": [
        "# example of a wgan for generating handwritten digits\n",
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from numpy.random import rand\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras import backend\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.constraints import Constraint\n",
        "from matplotlib import pyplot\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# constants\n",
        "# output dim of vgg16\n",
        "image_features_dim = 4096\n",
        "# output dim of doc2vec\n",
        "text_features_dim = 4096\n",
        "# output dim of word2vec\n",
        "class_features_dim = 4096\n",
        "# number of units in critic's layer 1\n",
        "critic_units_layer1 = 4096\n",
        "# number of units in generator layer1, 2\n",
        "generator_units_layer1 = 4096\n",
        "generator_units_layer2 = 4096\n",
        "# number of units in regressor layer1, 2, 3\n",
        "regressor_units_layer1 = 4096\n",
        "regressor_units_layer2 = 4096\n",
        "regressor_units_layer3 = 300\n",
        "# size of the latent space\n",
        "latent_dim = 4096 # for now it should be same as class_emb_dim\n",
        "\n",
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        "\n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}\n",
        "\n",
        "# calculate wasserstein loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "  # y_true.shape = (batch_size x 1)\n",
        "  # value of y_true is either 1 or -1 depending\n",
        "  # upon fake or real sample. y_pred is output of \n",
        "  # the critic module\n",
        "  return backend.mean(y_true * y_pred)\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  # generate points in the latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  # reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "# define the standalone critic model\n",
        "def define_critic(in_shape=(image_features_dim, )):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = ClipConstraint(0.01)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\t# downsample to 14x14\n",
        "\tmodel.add(Dense(critic_units_layer1, kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# scoring, linear activation\n",
        "\tmodel.add(Dense(1))\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)       # TODO: change it to adam\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "  # layer 1\n",
        "\tmodel.add(Dense(generator_units_layer1, kernel_initializer=init, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# layer 2\n",
        "\tmodel.add(Dense(generator_units_layer2, kernel_initializer=init))\n",
        "\tmodel.add(ReLU())\n",
        "\treturn model\n",
        "\n",
        "# define the standalone regressor model\n",
        "def define_regressor(in_shape=image_features_dim):\n",
        "  # weight initialization\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  # layer 1\n",
        "  model.add(Dense(regressor_units_layer1, kernel_initializer=init, input_dim=in_shape))\n",
        "  model.add(ReLU())\n",
        "  # layer 2\n",
        "  model.add(Dense(regressor_units_layer2, kernel_initializer=init))\n",
        "  model.add(ReLU())\n",
        "  # layer 3\n",
        "  model.add(Dense(regressor_units_layer3, kernel_initializer=init))\n",
        "  model.add(ReLU())\n",
        "  # model.compile will come in the final overall model\n",
        "  return model\n",
        "\n",
        "class mymodel(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(mymodel, self).__init__()\n",
        "    self.g1 = define_generator(latent_dim)\n",
        "    self.g2 = define_generator(latent_dim)\n",
        "    self.c1 = define_critic()\n",
        "    self.c2 = define_critic()\n",
        "    self.r1 = define_regressor()\n",
        "    self.r2 = define_regressor()\n",
        "\n",
        "  def call(self, _in):\n",
        "    in1, in2 = _in\n",
        "    g1c1 = self.g1(in1)\n",
        "    g1c1 = self.c1(g1c1)\n",
        "\n",
        "    g1r1 = self.g1(in1)\n",
        "    g1r1 = self.r1(g1r1)\n",
        "\n",
        "    g2r2 = self.g2(in2)\n",
        "    g2r2 = self.r2(g2r2)\n",
        "\n",
        "    g2c2 = self.g2(in2)\n",
        "    g2c2 = self.c2(g2c2)\n",
        "\n",
        "    return g1c1, g1r1, g2r2, g2c2\n",
        "\n",
        "# define the combined generator and critic model, for updating the generator\n",
        "def define_gan(generator, critic):\n",
        "\t# make weights in the critic not trainable\n",
        "\tfor layer in critic.layers:\n",
        "\t\tif not isinstance(layer, BatchNormalization):\n",
        "\t\t\tlayer.trainable = False\n",
        "\t# path 1\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(generator)\n",
        "\tmodel.add(critic)\n",
        " \n",
        "\t# compile model\n",
        "\t# opt = RMSprop(lr=0.00005)       #TODO: change it to adam\n",
        "\t# model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# load images\n",
        "def load_real_samples():\n",
        "  \"\"\" This function should return (nsamples x fetures) training data\n",
        "      for both image and text modality and corresponding class embeddings: \n",
        "      (nsamples x class_emb_size)\n",
        "  \"\"\"\n",
        "  return rand(1000, 4096), rand(1000, 4096), np.zeros((1000, 4096))\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(image_dataset, text_dataset, CE, n_samples):\n",
        "  \"\"\"\n",
        "    This function should return three things:\n",
        "    Xv : n_samples * features\n",
        "    Xt : n_samples * features\n",
        "    y : n_samples * 1\n",
        "    CE: n_samples * class_embedding_dimension\n",
        "  \"\"\"\n",
        "  # choose random instances\n",
        "  ix = randint(0, image_dataset.shape[0], n_samples)\n",
        "  # select images, text\n",
        "  Xv = image_dataset[ix]\n",
        "  Xt = text_dataset[ix]\n",
        "  # generate class labels, -1 for 'real'\n",
        "  y = -ones((n_samples, 1))\n",
        "  ce = CE[ix]\n",
        "  return Xv, Xt, y, ce\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator1, generator2, ce, latent_dim, n_samples):\n",
        "  # generate points in latent space for image generator\n",
        "  x_input_image = generate_latent_points(latent_dim, n_samples)\n",
        "  x_input_image += ce # adding the noise with class embeddings\n",
        "  # generate points in latent space for text generator\n",
        "  x_input_text = generate_latent_points(latent_dim, n_samples)\n",
        "  x_input_text += ce # noise + class_emb\n",
        "  # predict outputs\n",
        "  Xv = generator1.predict(x_input_image)\n",
        "  Xt = generator2.predict(x_input_text)\n",
        "  # create class labels with 1.0 for 'fake'\n",
        "  y = ones((n_samples, 1))\n",
        "  return Xv, Xt, y\n",
        "\n",
        "loss_objects = [wasserstein_loss, tf.nn.l2_loss, tf.keras.losses.KLD]\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "overall_model = mymodel(latent_dim)\n",
        "\n",
        "def train_step(inputs, y_true): # (inputs = (n1+ce, n2+ce))\n",
        "  with tf.GradientTape() as tape:\n",
        "    (g1c1, g1r1, g2r2, g2c2) = overall_model(inputs)\n",
        "    regressor_loss = loss_objects[1](g1r1, g2r2)\n",
        "    generator1_loss = loss_objects[0](y_true, g1c1)\n",
        "    generator2_loss = loss_objects[0](y_true, g2c2)\n",
        "    losses = [regressor_loss, generator1_loss, generator2_loss]\n",
        "  \n",
        "  gradients = tape.gradient(losses, overall_model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, overall_model.trainable_variables))\n",
        "  return (g1c1, g1r1, g2r2, g2c2)\n",
        "\n",
        "# train the generator and critic\n",
        "def train(g1, g2, c1, c2, r1, r2, image_dataset, text_dataset, CE, latent_dim, n_epochs=10, n_batch=64, n_critic=5):\n",
        "  bat_per_epo = int(image_dataset.shape[0] / n_batch)\n",
        "  n_steps = bat_per_epo * n_epochs\n",
        "  half_batch = int(n_batch / 2)\n",
        "  c11_hist, c12_hist, c21_hist, c22_hist= list(), list(), list(), list()\n",
        "  g1_loss, g2_loss = list(), list()\n",
        "\n",
        "  for i in range(n_steps):\n",
        "    # stores the loss values for both critics individually for real and fake data\n",
        "    c11_tmp, c12_tmp, c21_tmp, c22_tmp = list(), list(), list(), list()\n",
        "    for _ in range(n_critic):\n",
        "      # training both critics on real data\n",
        "      Xv_real, Xt_real, y_real, ce = generate_real_samples(image_dataset, text_dataset, CE, half_batch)\n",
        "      c_loss1 = c1.train_on_batch(Xv_real, y_real)\n",
        "      c11_tmp.append(c_loss1)\n",
        "      c_loss1 = c2.train_on_batch(Xt_real, y_real)\n",
        "      c21_tmp.append(c_loss1)\n",
        "\n",
        "      # training both critics on fake data\n",
        "      Xv_fake, Xt_fake, y_fake = generate_fake_samples(g1, g2, ce, latent_dim, half_batch) #TODO: how/where to use class_embs\n",
        "      c_loss1 = c1.train_on_batch(Xv_fake, y_fake)\n",
        "      c12_tmp.append(c_loss1)\n",
        "      c_loss1 = c2.train_on_batch(Xt_fake, y_fake)\n",
        "      c22_tmp.append(c_loss1)\n",
        "    # store critic loss\n",
        "    c11_hist.append(mean(c11_tmp))\n",
        "    c12_hist.append(mean(c12_tmp))\n",
        "    c21_hist.append(mean(c21_tmp))\n",
        "    c22_hist.append(mean(c22_tmp))\n",
        "\n",
        "    # prepare points in latent space as input for the generator\n",
        "    _, _, _, ce = generate_real_samples(image_dataset, text_dataset, CE, n_batch)\n",
        "    X_text = generate_latent_points(latent_dim, n_batch) + ce\n",
        "    X_image = generate_latent_points(latent_dim, n_batch) + ce\n",
        "    y_overall = -ones((n_batch, 1))\n",
        "\n",
        "    g1c1, g1r1, g2r2, g2c2 = train_step((X_image, X_text), y_overall)\n",
        "    g1_loss.append(wasserstein_loss(y_overall, g1c1))\n",
        "    g2_loss.append(wasserstein_loss(y_overall, g2c2))\n",
        "\n",
        "    # summarize loss on this batch\n",
        "    print('>%d, c11=%.3f, c12=%.3f, c21=%.3f, c22=%.3f, g1=%.3f, g2=%.3f' % (i+1, c11_hist[-1], c12_hist[-1], c21_hist[-1], c22_hist[-1], g1_loss[-1], g2_loss[-1]))\n",
        "  # line plots of loss\n",
        "  # plot_history(c1_hist, c2_hist, g_hist)\n",
        "\n",
        "# load image data\n",
        "image_dataset, text_dataset, CE = load_real_samples()\n",
        "# train model\n",
        "train(overall_model.g1, overall_model.g2, overall_model.c1, overall_model.c2, overall_model.r1, overall_model.r2, \n",
        "      image_dataset, text_dataset, CE, latent_dim)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_84/kernel:0', 'dense_84/bias:0', 'dense_85/kernel:0', 'dense_85/bias:0', 'dense_86/kernel:0', 'dense_86/bias:0'] when minimizing the loss.\n",
            ">1, c11=-1.583, c12=4.456, c21=-1.157, c22=4.336, g1=2.233, g2=2.497\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_84/kernel:0', 'dense_84/bias:0', 'dense_85/kernel:0', 'dense_85/bias:0', 'dense_86/kernel:0', 'dense_86/bias:0'] when minimizing the loss.\n",
            ">2, c11=-74.667, c12=68.157, c21=-69.447, c22=122.518, g1=-52.122, g2=-93.090\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_84/kernel:0', 'dense_84/bias:0', 'dense_85/kernel:0', 'dense_85/bias:0', 'dense_86/kernel:0', 'dense_86/bias:0'] when minimizing the loss.\n",
            ">3, c11=-140.961, c12=160.923, c21=-122.569, c22=361.266, g1=-130.202, g2=-288.309\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_84/kernel:0', 'dense_84/bias:0', 'dense_85/kernel:0', 'dense_85/bias:0', 'dense_86/kernel:0', 'dense_86/bias:0'] when minimizing the loss.\n",
            ">4, c11=-204.341, c12=294.038, c21=-169.601, c22=777.966, g1=-252.758, g2=-646.075\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_84/kernel:0', 'dense_84/bias:0', 'dense_85/kernel:0', 'dense_85/bias:0', 'dense_86/kernel:0', 'dense_86/bias:0'] when minimizing the loss.\n",
            ">5, c11=-266.157, c12=472.868, c21=-213.421, c22=1415.161, g1=-415.138, g2=-1211.467\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_84/kernel:0', 'dense_84/bias:0', 'dense_85/kernel:0', 'dense_85/bias:0', 'dense_86/kernel:0', 'dense_86/bias:0'] when minimizing the loss.\n",
            ">6, c11=-327.043, c12=699.173, c21=-255.197, c22=2268.455, g1=-635.805, g2=-2001.619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9de2a4448e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m train(overall_model.g1, overall_model.g2, overall_model.c1, overall_model.c2, overall_model.r1, overall_model.r2, \n\u001b[0;32m--> 276\u001b[0;31m       image_dataset, text_dataset, CE, latent_dim)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-9de2a4448e43>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(g1, g2, c1, c2, r1, r2, image_dataset, text_dataset, CE, latent_dim, n_epochs, n_batch, n_critic)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0;31m# training both critics on fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0mXv_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: how/where to use class_embs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       \u001b[0mc_loss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXv_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m       \u001b[0mc12_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_loss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0mc_loss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                                     class_weight)\n\u001b[1;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}